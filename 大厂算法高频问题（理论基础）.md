1.GBDT,XGBoost,LGBM的区别(阿里，头条)  
https://www.jianshu.com/p/765efe2b951a  
都属于boosting算法。GBDT是机器学习算法，XGBoost和LGBM是GBDT算法的算法实现。  
boosting基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，使得模型偏差bias不断降低。但不会显著降低方差，因为boosting训练过程使得各弱分类器之间是强相关的，缺乏独立性。  
gradient boosting算法：在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合，并计算出该分类器的权重，最终实现对模型的更新。（简单地说，就是建立一个新的模型来拟合未完全拟合真实样本的误差）  
gradient boosting decision tree GBDT  
GBDT保留了泰勒公式的一阶导，XGBoost保留了泰勒公式的二阶导。  
**如何用决策树来表示上一步的目标函数：**假设boosting的基模型用决策树实现，一棵好的决策树叶子节点是确定的。每一片叶子节点中样本的预测值是相同的，那么存在一个函数将【需要加入的新模型】中的每个样本映射到每个叶子节点上。  
**一般用贪心策略来优化：**  
a、从深度为0的树开始，对每个叶节点枚举所有的可用特征  
b、针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）  
c、选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集  
d、回到第1步，递归执行到满足特定条件为止  
**GBDT的算法可以总结为：**算法在拟合的每一步都生成一棵决策树，（拟合之前需要计算损失函数在每个样本上的一阶导和二阶导），通过贪心策略生成一棵树，计算每个叶子节点的G和H、计算预测值w，将新生成的决策树加入原模型，其中学习率是为了抑制模型的过拟合。  

GBDT是在决策树构建完成后再进行剪枝，而XGBoost是在决策树构建阶段就加入了正则项。XGBoost也有特定的准则来选取最优分裂（IC3,C4.5,CART；传统的GBDT采用CART自耦为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器）。通过最大化Gain差值来作为准则进行决策树的构建，通过遍历所有特征的所有取值，寻找使得损失函数前后相差最大时对应的分裂方式。  

XGBoost使用基于预排序的决策树算法，每遍历一个特征就需要计算一次特征的增益，时间复杂度为O(datafeature)。  
而LightGBM使用基于直方图的决策树算法，直方图的优化算法只需要计算K次，时间复杂度为O(Kfeature)。  

2.梯度下降法，牛顿法，拟牛顿法区别(阿里)  


3.SGD,ADAM区别(百度)  
梯度下降是目前神经网络使用最为广泛的优化算法之一。  
stochastic gradient descent SGD随机梯度下降  
adam利用梯度的一阶矩估计（一阶动量）和二阶矩（二阶动量）估计动态调整每个参数的学习率，经过偏置校正后，每一次迭代后的学习率都有确定范围，使得参数较为平稳。  


4.什么是梯度消失，饱和，如何改善(阿里)  


5.lr的推导(腾讯)  


6.SVM目标函数，为什么转为对偶(腾讯，百度)  


7.定义class mlp(头条)  


8.kd tree(腾讯)  


9.FFM的优化(百度)  


10.解释RESNET(百度，阿里)  


11.mapreduce思想(腾讯)  


12.解释BN(头条，百度)  


13.非结构化文本处理方法(阿里)  


14.bagging,boosting,stacking区别(阿里)  


15.CNN与RNN的区别(阿里)  


16.如何防止过拟合(头条)  



17. 类别不均衡如何处理  


18. 数据标准化有哪些方法/正则化如何实现/onehot原理  


19. 为什么XGB比GBDT好  


20. 数据清洗的方法有哪些/数据清洗步骤  


21. 缺失值填充方式有哪些  


22. 变量筛选有哪些方法  


23. 信息增益的计算公式  


24. 样本量很少情况下如何建模  


25. 交叉检验的实现  


26. 决策树如何剪枝  


27. WOE/IV值计算公式  


28. 分箱有哪些方法/分箱原理是什么  


29. 手推SVM：目标函数，计算逻辑，公式都写出来，平面与非平面  


30. 核函数有哪些  


31. XGB原理介绍/参数介绍/决策树原理介绍/决策树的优点  


32. Linux/C/Java熟悉程度  


33. 过拟合如何解决  


34. 平时通过什么渠道学习机器学习（好问题值得好好准备）  


35. 决策树先剪枝还是后剪枝好  


36. 损失函数有哪些  


37. 偏向做数据挖掘还是算法研究（好问题）  


38. bagging与boosting的区别  


39. 模型评估指标有哪些  


40. 解释模型复杂度/模型复杂度与什么有关  


41. 说出一个聚类算法  


42. ROC计算逻辑  


43. 如何判断一个模型中的变量太多  


44. 决策树与其他模型的损失函数、复杂度的比较  


45. 决策树能否有非数值型变量  


46. 决策树与神经网络的区别与优缺点对比  


47. 数据结构有哪些  


48. model ensembling的方法有哪些  


