1.gbdt,xgboost,lgbm的区别(阿里，头条)  
2.梯度下降法，牛顿法，拟牛顿法区别(阿里)  
3.SGD,ADAM区别(百度)  
4.什么是梯度消失，饱和，如何改善(阿里)  
5.lr的推导(腾讯)  
6.SVM目标函数，为什么转为对偶(腾讯，百度)  
7.定义class mlp(头条)  
8.kd tree(腾讯)  
9.FFM的优化(百度)  
10.解释RESNET(百度，阿里)  
11.mapreduce思想(腾讯)  
12.解释BN(头条，百度)  
13.非结构化文本处理方法(阿里)  
14.bagging.boosting.stacking区别(阿里)  
15.CNN与RNN的区别(阿里)  
16.如何防止过拟合(头条)  

17. 类别不均衡如何处理
18. 数据标准化有哪些方法/正则化如何实现/onehot原理
19. 为什么XGB比GBDT好
20. 数据清洗的方法有哪些/数据清洗步骤
21. 缺失值填充方式有哪些
22. 变量筛选有哪些方法
23. 信息增益的计算公式
24. 样本量很少情况下如何建模
25. 交叉检验的实现
26. 决策树如何剪枝
27. WOE/IV值计算公式
28. 分箱有哪些方法/分箱原理是什么
29. 手推SVM：目标函数，计算逻辑，公式都写出来，平面与非平面
30. 核函数有哪些
31. XGB原理介绍/参数介绍/决策树原理介绍/决策树的优点
32. Linux/C/Java熟悉程度
33. 过拟合如何解决
34. 平时通过什么渠道学习机器学习（好问题值得好好准备）
35. 决策树先剪枝还是后剪枝好
36. 损失函数有哪些
37. 偏向做数据挖掘还是算法研究（好问题）
38. bagging与boosting的区别
39. 模型评估指标有哪些
40. 解释模型复杂度/模型复杂度与什么有关
41. 说出一个聚类算法
42. ROC计算逻辑
43. 如何判断一个模型中的变量太多
44. 决策树与其他模型的损失函数、复杂度的比较
45. 决策树能否有非数值型变量
46. 决策树与神经网络的区别与优缺点对比
47. 数据结构有哪些
48. model ensembling的方法有哪些
